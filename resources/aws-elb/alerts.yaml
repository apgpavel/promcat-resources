---
apiVersion: v1
kind: Alert
app: "AWS ELB"
version: 1.0.0
appVersion: 
  - 1.0.0
maintainers:
  - name: Sysdig team
    link: https://sysdig.com/
description: |
  # Alerts
  ## High4XXErrorsFromLoadBalancer and High5XXErrorsFromLoadBalancer
  A request to a load balancer can end in an error triggered directly by the load balancer with codes stating by either by 4 or 5.
  For a list of possible error codes and their causes you can read the [load balancers error codes documentation](https://docs.aws.amazon.com/elasticloadbalancing/atest/application/load-balancer-troubleshooting.html). 
  
  This alert triggers when the percentage of requests with each error code excess a threshold for a certain period of time. 
  The recommended value for this alert is 15% of the requests ended with error for 10 minutes.
  
  ## High4XXErrorsFromBackend and High5XXErrorsFromBackend
  A request to a load balancer can end in an error triggered by the backend with codes stating by either by 4 or 5.
  For a list of possible error codes and their causes you can read the [load balancers error codes documentation](https://docs.aws.amazon.com/elasticloadbalancing/atest/application/load-balancer-troubleshooting.html). 
  
  This alert triggers when the percentage of requests with each error code excess a threshold for a certain period of time. 
  The recommended value for this alert is 15% of the requests ended with error for 10 minutes.
  
  ## UnhealthyHostInLoadBalancer
  This alerts triggers when there is one or more unhealthy hosts in a load balancer.
  
  ## QueueSpilloverRejections
  This alert triggers when there are continuos rejected connections due to saturation in teh queue in a load balancer for more than 10 minutes.

  ## RejectedConnectionsInLoadBalancer
  This alert triggers when there are continuos rejected connections in a load balancer for more than 10 minutes. 
   
  ## HighLatencyInLoadBalancer
  The latency time is the time elapsed, in seconds, after the request leaves the load balancer until a response from the backend is received. 
  
  This alerts triggers when the percentile 95 of a load balancer is over 250ms for 10 minutes. 
  
  # Installing the alerts
  ## In Prometheus
  Copy the alerts for Prometheus and paste them inside of the configuration file in the groups section:
  ```yaml
  [...]
  groups:
  - name: example
      # Insert here the alerts
      rules: 
  ``` 
  ## In Sysdig Monitor
  To install these alerts in Sysdig Monitor, contact with the support service. 
configurations: 
  - kind: Prometheus
    data: |
      - alert: High4XXErrorsFromLoadBalancer
        expr: |
          aws_elb_httpcode_elb_4_xx_count_sum / aws_elb_request_count_sum 
          > 0.15
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: High 4XX error rate in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: High5XXErrorsFromLoadBalancer
        expr: |
          aws_elb_httpcode_elb_5_xx_count_sum / aws_elb_request_count_sum  
          > 0.15
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: High 5XX error rate in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: High4XXErrorsFromBackend
        expr: |
          aws_elb_httpcode_backend_4_xx_count_sum / aws_elb_request_count_sum 
          > 0.15
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: High 4XX error rate in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: High5XXErrorsFromBackend
        expr: |
          aws_elb_httpcode_backend_5_xx_count_sum / aws_elb_request_count_sum  
          > 0.15
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: High 5XX error rate in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: UnhealthyHostInLoadBalancer
        expr: aws_elb_un_healthy_host_count_minimum > 0
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: Unhealthy host detected in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: QueueSpilloverRejections
        expr: aws_elb_spillover_count_sum > 0
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: Recurrent queue spillover rejections in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: RejectedConnectionsInLoadBalancer
        expr: aws_alb_rejected_connection_count_sum > 0
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: Rejected connections in load balancer {{$labels.dimension_LoadBalancerName}}
      - alert: HighLatencyInLoadBalancer
        expr: aws_elb_latency_p95 > 0.250
        for: 10m
        labels: 
          severity: page
        annotations:
          summary: High latency in load balancer {{$labels.dimension_LoadBalancerName}} 
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_httpcode_elb_4_xx_count_sum / aws_elb_request_count_sum \n> 0.15\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "High4XXErrorsFromLoadBalancer",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_httpcode_elb_5_xx_count_sum / aws_elb_request_count_sum  \n> 0.15\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "High5XXErrorsFromLoadBalancer",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_httpcode_backend_4_xx_count_sum / aws_elb_request_count_sum \n> 0.15\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "High4XXErrorsFromBackend",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_httpcode_backend_5_xx_count_sum / aws_elb_request_count_sum  \n> 0.15\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "High5XXErrorsFromBackend",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_un_healthy_host_count_minimum > 0",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "UnhealthyHostInLoadBalancer",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_spillover_count_sum > 0",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "QueueSpilloverRejections",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_alb_rejected_connection_count_sum > 0",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "RejectedConnectionsInLoadBalancer",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "aws_elb_latency_p95 > 0.250",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "HighLatencyInLoadBalancer",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
